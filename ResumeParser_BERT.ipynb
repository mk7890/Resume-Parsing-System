{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfYriCz9I7H1jTbnvTI2Pi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8f175bee01e54171964462a39acbff80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20f102108e4d41c9bd5dbc85381ff6f4",
              "IPY_MODEL_bb7b962d827f4fc19be619b87b4526e5",
              "IPY_MODEL_615d0059c4864489bb209afb39ef0637"
            ],
            "layout": "IPY_MODEL_3cad46a1ad274115b435100a56ad7a04"
          }
        },
        "20f102108e4d41c9bd5dbc85381ff6f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d30ed96393724f0dba0957e0995aa790",
            "placeholder": "​",
            "style": "IPY_MODEL_082bba2f93e548a18534dad97ec4c511",
            "value": "Map: 100%"
          }
        },
        "bb7b962d827f4fc19be619b87b4526e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af1b508a7fac49bba7fc215f3d6714c7",
            "max": 25986,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e84e059f42f74eb19a4fb0b82d2c11c1",
            "value": 25986
          }
        },
        "615d0059c4864489bb209afb39ef0637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bc909535b5640e98e4594a91dc64117",
            "placeholder": "​",
            "style": "IPY_MODEL_d190f659e6424374a859e77209371c74",
            "value": " 25986/25986 [00:32&lt;00:00, 858.27 examples/s]"
          }
        },
        "3cad46a1ad274115b435100a56ad7a04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d30ed96393724f0dba0957e0995aa790": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "082bba2f93e548a18534dad97ec4c511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af1b508a7fac49bba7fc215f3d6714c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e84e059f42f74eb19a4fb0b82d2c11c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2bc909535b5640e98e4594a91dc64117": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d190f659e6424374a859e77209371c74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28519e00829b485a90ecb158a91dc393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8740c5738a704a58bd9df8c11fe58053",
              "IPY_MODEL_2dbf9294e38d4d99b297991b427e32de",
              "IPY_MODEL_ce9f75b0e0244c5ca2c1bb3bb5c14fac"
            ],
            "layout": "IPY_MODEL_f6376c8ec7b94869905733a3e53db1ec"
          }
        },
        "8740c5738a704a58bd9df8c11fe58053": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3ab3550e08a41bdae74515ad801bfe5",
            "placeholder": "​",
            "style": "IPY_MODEL_1d7f849d073b4eedb23e35c553aaf387",
            "value": "Map: 100%"
          }
        },
        "2dbf9294e38d4d99b297991b427e32de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d448d025dac435f86a83a2cd6483d07",
            "max": 6497,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_171bac7a46c748038fc2c92a18464d29",
            "value": 6497
          }
        },
        "ce9f75b0e0244c5ca2c1bb3bb5c14fac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbdf69434cae41b5a45c4626a48b9c8b",
            "placeholder": "​",
            "style": "IPY_MODEL_9ea8e6bc257240fcb8a4fdb3dd0712d9",
            "value": " 6497/6497 [00:07&lt;00:00, 779.05 examples/s]"
          }
        },
        "f6376c8ec7b94869905733a3e53db1ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3ab3550e08a41bdae74515ad801bfe5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d7f849d073b4eedb23e35c553aaf387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d448d025dac435f86a83a2cd6483d07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "171bac7a46c748038fc2c92a18464d29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dbdf69434cae41b5a45c4626a48b9c8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ea8e6bc257240fcb8a4fdb3dd0712d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mk7890/Resume-Parsing-System/blob/main/ResumeParser_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Data Collection\n",
        "Collect a variety of resumes in PDF format. You'll need a diverse dataset to ensure your model can generalize well.\n",
        "\n",
        "2. Preprocessing\n",
        "Convert PDF to text: You can use libraries like PyMuPDF or pdfminer.\n",
        "\n",
        "Clean the text: Remove unnecessary characters and normalize the text.\n",
        "\n",
        "3. Feature Extraction\n",
        "Tokenization: Split the text into individual words or tokens.\n",
        "\n",
        "Named Entity Recognition (NER): Use NER to identify and classify entities in the text. Libraries like spaCy are excellent for this task.\n",
        "\n",
        "Regular Expressions: For identifying specific patterns like phone numbers and emails.\n",
        "\n",
        "4. Building the Model\n",
        "Use a pre-trained language model like BERT or fine-tune it for your specific use case.\n",
        "\n",
        "Train the model on annotated resumes where entities like name, job role, etc., are labeled.\n",
        "\n",
        "5. Model Evaluation\n",
        "Use metrics like precision, recall, and F1-score to evaluate your model's performance.\n",
        "\n",
        "6. Saving and Deployment\n",
        "Save the trained model using a library like joblib or pickle.\n",
        "\n",
        "Deploy the model using Streamlit for an interactive web application."
      ],
      "metadata": {
        "id": "ee70uGO8v5ZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Libraries"
      ],
      "metadata": {
        "id": "J0w7siczwB8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "u-S7c-ySwhiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDF to Text Conversion"
      ],
      "metadata": {
        "id": "2_1Mzk3FwGG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjG7Ed-tw4Pr",
        "outputId": "d683de4c-0def-4ca0-d0b1-a451c91940dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.25.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy transformers pdfplumber joblib pickle5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qJzzA3isC5t1",
        "outputId": "8ee9b080-5925-4ef9-d00c-0c1b7daa6a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Collecting pdfplumber\n",
            "  Using cached pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Collecting pickle5\n",
            "  Using cached pickle5-0.0.11.tar.gz (132 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Using cached pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Using cached pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Using cached pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
            "Using cached pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "Using cached pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "Building wheels for collected packages: pickle5\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pickle5 (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pickle5\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for pickle5\n",
            "Failed to build pickle5\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pickle5)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_D524fNFiJc",
        "outputId": "6ff6a741-e12c-4414-d898-c4af8593a574"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.5 pypdfium2-4.30.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Collection & Preprocessing"
      ],
      "metadata": {
        "id": "AmYJhrXMD0XS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Resume Dataset"
      ],
      "metadata": {
        "id": "14YSJrizvUzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resume_data = pd.read_csv(\"/content/ner_filled_clean_resume_dataset.csv\")"
      ],
      "metadata": {
        "id": "yZimjE18vUD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "GXFYEFeZvdTd",
        "outputId": "22091bb6-c242-4875-87ec-0949f3248ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Job Title   Applicant Name              Phone  \\\n",
              "0       Social Media Manager  Johnny Davidson   +49-101-66386554   \n",
              "1     Frontend Web Developer      Amanda Owen    +1-337-912-4766   \n",
              "2    Quality Control Manager        John Lowe    +1-253-734-6013   \n",
              "3  Wireless Network Engineer    David Spencer    +44-8358-811442   \n",
              "4         Conference Manager       Jade Lopez  +33-6-42-03-88-59   \n",
              "\n",
              "                          Email                             Linkedin Address  \\\n",
              "0     johnny.davidson@yahoo.com  https://www.linkedin.com/in/johnny-davidson   \n",
              "1    amanda.owen@protonmail.com      https://www.linkedin.com/in/amanda-owen   \n",
              "2           john.lowe@yahoo.com        https://www.linkedin.com/in/john-lowe   \n",
              "3  david.spencer@protonmail.com    https://www.linkedin.com/in/david-spencer   \n",
              "4          jade.lopez@gmail.com       https://www.linkedin.com/in/jade-lopez   \n",
              "\n",
              "   Years of Work Experience    Skills  \\\n",
              "0                        16        []   \n",
              "1                        26  ['Java']   \n",
              "2                        39        []   \n",
              "3                        14        []   \n",
              "4                        25        []   \n",
              "\n",
              "                                Companies Worked For Education Background  \\\n",
              "0  ['Digital Marketing Specialist', 'Facebook', '...                   []   \n",
              "1                                ['BCA', 'BCA', 'U']                   []   \n",
              "2                                     ['Operations']                   []   \n",
              "3                   ['Network Engineer', 'Wireless']                   []   \n",
              "4                                            ['MBA']                   []   \n",
              "\n",
              "  Education Institutions Attended Certifications  \\\n",
              "0                              []             []   \n",
              "1                              []             []   \n",
              "2                              []             []   \n",
              "3                              []             []   \n",
              "4                              []             []   \n",
              "\n",
              "                              Physical Address  \n",
              "0  East Josephstad, Slovakia (Slovak Republic)  \n",
              "1    Lake Paulmouth, Saint Pierre and Miquelon  \n",
              "2                       Patriciaville, Iceland  \n",
              "3                South Vincent, American Samoa  \n",
              "4                      Port Brittney, Malaysia  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f1ab1c29-2e9e-48ac-bcb7-870e51bafbb1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Job Title</th>\n",
              "      <th>Applicant Name</th>\n",
              "      <th>Phone</th>\n",
              "      <th>Email</th>\n",
              "      <th>Linkedin Address</th>\n",
              "      <th>Years of Work Experience</th>\n",
              "      <th>Skills</th>\n",
              "      <th>Companies Worked For</th>\n",
              "      <th>Education Background</th>\n",
              "      <th>Education Institutions Attended</th>\n",
              "      <th>Certifications</th>\n",
              "      <th>Physical Address</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Social Media Manager</td>\n",
              "      <td>Johnny Davidson</td>\n",
              "      <td>+49-101-66386554</td>\n",
              "      <td>johnny.davidson@yahoo.com</td>\n",
              "      <td>https://www.linkedin.com/in/johnny-davidson</td>\n",
              "      <td>16</td>\n",
              "      <td>[]</td>\n",
              "      <td>['Digital Marketing Specialist', 'Facebook', '...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>East Josephstad, Slovakia (Slovak Republic)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Frontend Web Developer</td>\n",
              "      <td>Amanda Owen</td>\n",
              "      <td>+1-337-912-4766</td>\n",
              "      <td>amanda.owen@protonmail.com</td>\n",
              "      <td>https://www.linkedin.com/in/amanda-owen</td>\n",
              "      <td>26</td>\n",
              "      <td>['Java']</td>\n",
              "      <td>['BCA', 'BCA', 'U']</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Lake Paulmouth, Saint Pierre and Miquelon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Quality Control Manager</td>\n",
              "      <td>John Lowe</td>\n",
              "      <td>+1-253-734-6013</td>\n",
              "      <td>john.lowe@yahoo.com</td>\n",
              "      <td>https://www.linkedin.com/in/john-lowe</td>\n",
              "      <td>39</td>\n",
              "      <td>[]</td>\n",
              "      <td>['Operations']</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Patriciaville, Iceland</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Wireless Network Engineer</td>\n",
              "      <td>David Spencer</td>\n",
              "      <td>+44-8358-811442</td>\n",
              "      <td>david.spencer@protonmail.com</td>\n",
              "      <td>https://www.linkedin.com/in/david-spencer</td>\n",
              "      <td>14</td>\n",
              "      <td>[]</td>\n",
              "      <td>['Network Engineer', 'Wireless']</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>South Vincent, American Samoa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Conference Manager</td>\n",
              "      <td>Jade Lopez</td>\n",
              "      <td>+33-6-42-03-88-59</td>\n",
              "      <td>jade.lopez@gmail.com</td>\n",
              "      <td>https://www.linkedin.com/in/jade-lopez</td>\n",
              "      <td>25</td>\n",
              "      <td>[]</td>\n",
              "      <td>['MBA']</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>Port Brittney, Malaysia</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f1ab1c29-2e9e-48ac-bcb7-870e51bafbb1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f1ab1c29-2e9e-48ac-bcb7-870e51bafbb1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f1ab1c29-2e9e-48ac-bcb7-870e51bafbb1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-311e56c2-f464-4087-be3b-04c914f6c171\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-311e56c2-f464-4087-be3b-04c914f6c171')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-311e56c2-f464-4087-be3b-04c914f6c171 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "resume_data",
              "summary": "{\n  \"name\": \"resume_data\",\n  \"rows\": 32483,\n  \"fields\": [\n    {\n      \"column\": \"Job Title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 400,\n        \"samples\": [\n          \"SEO Copywriter\",\n          \"Environmental Consultant\",\n          \"Lighting Designer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Applicant Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 26617,\n        \"samples\": [\n          \"Cory Snow PhD\",\n          \"Ricardo Dennis\",\n          \"Mr. James Lang\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Phone\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 32483,\n        \"samples\": [\n          \"+1-535-982-1764\",\n          \"+49-200-88524072\",\n          \"+1-470-298-9597\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Email\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29785,\n        \"samples\": [\n          \"madeline.hernandez@gmail.com\",\n          \"stacy.perez@yahoo.com\",\n          \"alejandro.klein.md@yahoo.com\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Linkedin Address\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 26612,\n        \"samples\": [\n          \"https://www.linkedin.com/in/tara-neal\",\n          \"https://www.linkedin.com/in/lee-allen\",\n          \"https://www.linkedin.com/in/jeffrey-rodriguez\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Years of Work Experience\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 0,\n        \"max\": 40,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          3,\n          38,\n          36\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Skills\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 23,\n        \"samples\": [\n          \"['Python', 'Machine Learning', 'SQL']\",\n          \"['SQL', 'Java']\",\n          \"[]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Companies Worked For\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4990,\n        \"samples\": [\n          \"['Disabilty Insurance Agent', 'Washington Technical and Soft Skill', 'Analysis Cross', 'MS Windows MS Office', 'Sagitta BenefitPoint CC Pulse Verint Impact', 'Cisco Enterprise', 'American Modern Chubb', 'Kemper RLI Mapfre MetLife Mutual of Enumclaw Progressive Safeco Travelers Premera Blue Cross', 'Cigna Kaiser Permanente Group', 'Symetra', 'Company', 'Licensed Life and Disability Benefit', 'Assist Consultants', 'Monitor department', 'City State', 'Licensed Property Casualty Customer Service Agents', 'Monitor department', 'Customer Service', 'City State Assist', 'Customer Service Agents Monitor', 'Education Bachelor of Social Sciences University', 'the National Alliance for Insurance Education and Research Current Certified Insurance Councilor Agency Management Life Health Commercial Property Goal', 'Personal Lines', 'CIC', 'Agency', 'Customer Service Customer Care', 'Excel MS Office', 'Project Management', 'Research', 'C', '##ualty Life', 'Disabil', 'Technical', 'Soft Skill', 'Materials', 'Assu', 'As', 'Epic V', '##af', 'Sagitta Benef', '##Point', '##ron', '##S', 'Cisco Enterprise Finesse', 'Excel', 'Dairyl', 'Kemp', 'Mutual', '##c', '##co', 'Pre', 'Blue Cross Reg', 'Blue Shield United Healthcare Aetna Cigna Kaiser Permanente Group', 'Symetra Unum Delta Dental Willamette Dental', 'VSP', 'City State', 'Advocate']\",\n          \"['Classroom management Curriculum development Student', 'B']\",\n          \"['SR campus recruiter Summary Solutions', 'Policy Development Organization', 'Project Management Social', 'Behavioral Interviews EEO', 'Adobe Connect Interviews Manage and', 'Peak Season Travel Recruitment Planning', 'City State', 'Develop', 'NC SC', 'City State Manage', 'Instructional Coaches', 'City State', 'TTF', 'Sprint Sealy Aetna', 'BU', 'TTF', 'TTF', 'PeopleSoft Education ma Human Resources', 'Webster University City State Human Resources ba Education South Carolina State University City State Technical Skills Microsoft Office', 'SR', 'Summary Solutions', 'Policy Development Organization', 'Project Management', 'EEO', 'OFFC', '##pe', 'Adobe Connect', 'City State', 'DMA', 'City State Man', 'City State', 'Sprint', '##IC']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Education Background\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"[]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Education Institutions Attended\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"[]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Certifications\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 923,\n        \"samples\": [\n          \"['Iowa State University City State will be a Certified Financial Planner after completion GPA GPA GPA Marketing Iowa State University City State Marketing Microsoft Front Page Certified New Horizons Des Moines IA a ccomplishment l roll out the first company wide intranet for a Fortune Company l learn two computer language on my own to well understand the limit of what developer can do l in elect Professional and scientific representative l implement entire electronic thesis dissertation solution at Iowa State University l publish paper at International Academy of Technology Education and Development IATED conference Empowering Departments across the University by use Web Technologies']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Physical Address\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30502,\n        \"samples\": [\n          \"Brownbury, Greece\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlJ0OH--vgPu",
        "outputId": "f309ce93-c4d4-4ebe-ca59-1a5490ec7b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 32483 entries, 0 to 32482\n",
            "Data columns (total 12 columns):\n",
            " #   Column                           Non-Null Count  Dtype \n",
            "---  ------                           --------------  ----- \n",
            " 0   Job Title                        32483 non-null  object\n",
            " 1   Applicant Name                   32483 non-null  object\n",
            " 2   Phone                            32483 non-null  object\n",
            " 3   Email                            32483 non-null  object\n",
            " 4   Linkedin Address                 32483 non-null  object\n",
            " 5   Years of Work Experience         32483 non-null  int64 \n",
            " 6   Skills                           32483 non-null  object\n",
            " 7   Companies Worked For             32483 non-null  object\n",
            " 8   Education Background             32483 non-null  object\n",
            " 9   Education Institutions Attended  32483 non-null  object\n",
            " 10  Certifications                   32483 non-null  object\n",
            " 11  Physical Address                 32483 non-null  object\n",
            "dtypes: int64(1), object(11)\n",
            "memory usage: 3.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8UVw_hMvjfU",
        "outputId": "5019aa91-c04c-4c5f-debc-21be83791b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Job Title', 'Applicant Name', 'Phone', 'Email', 'Linkedin Address',\n",
              "       'Years of Work Experience', 'Skills', 'Companies Worked For',\n",
              "       'Education Background', 'Education Institutions Attended',\n",
              "       'Certifications', 'Physical Address'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Skills and Job Role Data\n",
        "\n",
        "def load_keywords(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return [line.strip().lower() for line in file.readlines()]\n",
        "\n",
        "skills_list = load_keywords(\"/content/unique_skills.txt\")\n",
        "job_roles_list = load_keywords(\"/content/unique_job_roles.txt\")"
      ],
      "metadata": {
        "id": "3q_I1XtqD1oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NER (Named Entity Recognition)\n",
        "\n",
        "Fine-tune spaCy's NER model to extract required entities.\n",
        "\n",
        "Train a Custom NER Model\n",
        "\n",
        "Prepare Training Data\n",
        "\n",
        "## Convert CSV Data to NER Format"
      ],
      "metadata": {
        "id": "S-fn2KgpwXVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load SpaCy's English tokenizer\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Define entity labels\n",
        "LABELS = {\n",
        "    \"Job Title\": \"JOB\",\n",
        "    \"Applicant Name\": \"NAME\",\n",
        "    \"Phone\": \"PHONE\",\n",
        "    \"Email\": \"EMAIL\",\n",
        "    \"Linkedin Address\": \"LINKEDIN\",\n",
        "    \"Years of Work Experience\": \"EXPERIENCE\",\n",
        "    \"Skills\": \"SKILL\",\n",
        "    \"Companies Worked For\": \"COMPANY\",\n",
        "    \"Education Background\": \"EDUCATION\",\n",
        "    \"Education Institutions Attended\": \"INSTITUTION\",\n",
        "    \"Certifications\": \"CERTIFICATION\",\n",
        "    \"Physical Address\": \"ADDRESS\",\n",
        "}\n",
        "\n",
        "# Load the CSV file\n",
        "df = resume_data\n",
        "\n",
        "# Convert to NER format\n",
        "annotated_data = []\n",
        "\n",
        "def convert_to_ner(sentence, row, entity_dict):\n",
        "    \"\"\"\n",
        "    Converts a sentence into BIO format based on row entities.\n",
        "    \"\"\"\n",
        "    doc = nlp(sentence)\n",
        "    bio_tags = [\"O\"] * len(doc)\n",
        "\n",
        "    for column, entity in entity_dict.items():\n",
        "        entity_value = str(row[column]) if pd.notna(row[column]) else None\n",
        "        if entity_value:\n",
        "            words = entity_value.split()\n",
        "            for i in range(len(doc)):\n",
        "                if doc[i].text == words[0]:  # First word match\n",
        "                    bio_tags[i] = f\"B-{entity}\"\n",
        "                    for j in range(1, len(words)):  # Remaining words as I-entity\n",
        "                        if i + j < len(doc) and doc[i + j].text == words[j]:\n",
        "                            bio_tags[i + j] = f\"I-{entity}\"\n",
        "\n",
        "    return [(token.text, tag) for token, tag in zip(doc, bio_tags)]\n",
        "\n",
        "# Process each row in the dataset\n",
        "for _, row in df.iterrows():\n",
        "    sentence = \" \".join(row.dropna().astype(str).values)  # Combine all fields into one text\n",
        "    labeled_tokens = convert_to_ner(sentence, row, LABELS)\n",
        "    annotated_data.append(labeled_tokens)\n",
        "\n",
        "# Save to a text file in NER format\n",
        "with open(\"annotated_ner_dataset.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for sentence in annotated_data:\n",
        "        for word, tag in sentence:\n",
        "            f.write(f\"{word} {tag}\\n\")\n",
        "        f.write(\"\\n\")  # Sentence boundary\n",
        "\n",
        "print(\"✅ NER dataset saved as 'annotated_ner_dataset.txt'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA1SH3U3xB0N",
        "outputId": "5cf606c7-bba0-46ef-b2a3-b9ddc31d6d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ NER dataset saved as 'annotated_ner_dataset.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert BIO format to Hugging Face's Token Classification Format"
      ],
      "metadata": {
        "id": "w96NvVIgy4Sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the BIO-formatted dataset\n",
        "bio_sentences = []\n",
        "sentence = []\n",
        "with open(\"annotated_ner_dataset.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        if line.strip():  # If line is not empty\n",
        "            word, tag = line.strip().split()\n",
        "            sentence.append((word, tag))\n",
        "        else:  # Sentence boundary\n",
        "            if sentence:\n",
        "                bio_sentences.append(sentence)\n",
        "                sentence = []\n",
        "\n",
        "# Convert to Hugging Face format\n",
        "hf_data = []\n",
        "for sentence in bio_sentences:\n",
        "    tokens, labels = zip(*sentence)  # Unzip words and tags\n",
        "    hf_data.append({\"tokens\": list(tokens), \"labels\": list(labels)})\n",
        "\n",
        "# Save as JSON\n",
        "with open(\"hf_ner_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(hf_data, f, indent=4)\n",
        "\n",
        "print(\"✅ Hugging Face token classification dataset saved as 'hf_ner_dataset.json'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNmR8az_zM-H",
        "outputId": "5251ccc3-b06b-420e-a51d-b458372754bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Hugging Face token classification dataset saved as 'hf_ner_dataset.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the JSON dataset\n",
        "with open(\"hf_ner_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Print the first 3 examples for preview\n",
        "print(\"🔍 Preview of Hugging Face NER Dataset:\")\n",
        "for i, sample in enumerate(data[:3]):  # Show first 3 samples\n",
        "    print(f\"\\nSample {i + 1}:\")\n",
        "    print(\"Tokens:\", sample[\"tokens\"])\n",
        "    print(\"Labels:\", sample[\"labels\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qihR-47zzhBO",
        "outputId": "831f0322-5f73-4eb0-ea8c-8d661ae19b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Preview of Hugging Face NER Dataset:\n",
            "\n",
            "Sample 1:\n",
            "Tokens: ['Social', 'Media', 'Manager', 'Johnny', 'Davidson', '+49', '-', '101', '-', '66386554', 'johnny.davidson@yahoo.com', 'https://www.linkedin.com/in/johnny-davidson', '16', '[', ']', '[', \"'\", 'Digital', 'Marketing', 'Specialist', \"'\", ',', \"'\", 'Facebook', \"'\", ',', \"'\", 'Facebook', \"'\", ']', '[', ']', '[', ']', '[', ']', 'East', 'Josephstad', ',', 'Slovakia', '(', 'Slovak', 'Republic', ')']\n",
            "Labels: ['B-JOB', 'I-JOB', 'I-JOB', 'B-NAME', 'I-NAME', 'O', 'O', 'O', 'O', 'O', 'B-EMAIL', 'B-LINKEDIN', 'B-EXPERIENCE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ADDRESS', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "\n",
            "Sample 2:\n",
            "Tokens: ['Frontend', 'Web', 'Developer', 'Amanda', 'Owen', '+1', '-', '337', '-', '912', '-', '4766', 'amanda.owen@protonmail.com', 'https://www.linkedin.com/in/amanda-owen', '26', '[', \"'\", 'Java', \"'\", ']', '[', \"'\", 'BCA', \"'\", ',', \"'\", 'BCA', \"'\", ',', \"'\", 'U', \"'\", ']', '[', ']', '[', ']', '[', ']', 'Lake', 'Paulmouth', ',', 'Saint', 'Pierre', 'and', 'Miquelon']\n",
            "Labels: ['B-JOB', 'I-JOB', 'I-JOB', 'B-NAME', 'I-NAME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EMAIL', 'B-LINKEDIN', 'B-EXPERIENCE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ADDRESS', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "\n",
            "Sample 3:\n",
            "Tokens: ['Quality', 'Control', 'Manager', 'John', 'Lowe', '+1', '-', '253', '-', '734', '-', '6013', 'john.lowe@yahoo.com', 'https://www.linkedin.com/in/john-lowe', '39', '[', ']', '[', \"'\", 'Operations', \"'\", ']', '[', ']', '[', ']', '[', ']', 'Patriciaville', ',', 'Iceland']\n",
            "Labels: ['B-JOB', 'I-JOB', 'I-JOB', 'B-NAME', 'I-NAME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EMAIL', 'B-LINKEDIN', 'B-EXPERIENCE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a BERT Token Classification Model for Resume Parsing\n",
        "Now that we have the dataset in Hugging Face’s token classification format, we can fine-tune a BERT model for Named Entity Recognition (NER).\n",
        "\n",
        "\n",
        "Steps for Training\n",
        "\n",
        "Load the NER dataset (hf_ner_dataset.json).\n",
        "\n",
        "Tokenize the data using a pre-trained BERT tokenizer.\n",
        "\n",
        "Convert labels to numerical IDs.\n",
        "\n",
        "Fine-tune a bert-base-uncased model using the\n",
        "\n",
        "Hugging Face transformers library.\n",
        "\n",
        "Evaluate on a test set."
      ],
      "metadata": {
        "id": "qGt9FHuG0M_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch scikit-learn seqeval\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XVcBS1Z-0Sm5",
        "outputId": "67b0afae-7792-4c4a-c396-6e84894a1f81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=97bb3e1a9be43b6a484041fdf445f9def74db770f13b02d94c299a110088051e\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
            "Successfully built seqeval\n",
            "Installing collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, seqeval, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 seqeval-1.2.2 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ✅ Step 1: Load Dataset\n",
        "with open(\"hf_ner_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# ✅ Step 2: Define Unique Label Set\n",
        "unique_labels = sorted(set(label for sample in data for label in sample[\"labels\"]))\n",
        "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "print(\"Label Mapping:\", label2id)\n",
        "\n",
        "# ✅ Step 3: Tokenizer Setup\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# ✅ Step 4: Tokenization & Label Alignment\n",
        "def tokenize_and_align_labels(samples):\n",
        "    tokenized_inputs = tokenizer(samples[\"tokens\"], truncation=True, is_split_into_words=True, padding=True)\n",
        "\n",
        "    all_labels = []\n",
        "    for i, word_ids in enumerate(tokenized_inputs.word_ids(batch_index=i) for i in range(len(samples[\"tokens\"]))):\n",
        "        aligned_labels = []\n",
        "        previous_word_idx = None\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                aligned_labels.append(-100)  # Ignore special tokens\n",
        "            elif word_idx != previous_word_idx:\n",
        "                aligned_labels.append(label2id[samples[\"labels\"][i][word_idx]])  # First sub-token\n",
        "            else:\n",
        "                aligned_labels.append(label2id[samples[\"labels\"][i][word_idx]])  # Remaining sub-tokens\n",
        "            previous_word_idx = word_idx\n",
        "        all_labels.append(aligned_labels)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = all_labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# ✅ Step 5: Convert to Hugging Face Dataset\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "hf_dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_list(train_data),\n",
        "    \"test\": Dataset.from_list(test_data)\n",
        "})\n",
        "\n",
        "# ✅ Step 6: Apply Tokenization with Batched Processing\n",
        "hf_dataset = hf_dataset.map(tokenize_and_align_labels, batched=True, remove_columns=[\"tokens\", \"labels\"])\n",
        "\n",
        "# ✅ Step 7: Load Pretrained Model\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=len(label2id),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# ✅ Step 8: Training Arguments (Optimized for 30-minute Training)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert-ner-model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=0.01,  # Slightly lower for stability\n",
        "    per_device_train_batch_size=16,  # Increased for faster training\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=1,  # Reduced epochs\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,  # Reduced logging overhead\n",
        "    save_total_limit=2,\n",
        "    fp16=True,  # Mixed precision for speed-up\n",
        "    dataloader_pin_memory=True,  # Faster GPU memory transfers\n",
        ")\n",
        "\n",
        "# ✅ Step 9: Define Data Collator\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "# ✅ Step 10: Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=hf_dataset[\"train\"],\n",
        "    eval_dataset=hf_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# ✅ Step 11: Train Model\n",
        "trainer.train()\n",
        "\n",
        "# ✅ Step 12: Save Model & Tokenizer\n",
        "model.save_pretrained(\"./bert-ner-model\")\n",
        "tokenizer.save_pretrained(\"./bert-ner-model\")\n",
        "\n",
        "print(\"🎯 Training Complete! Model Saved at './bert-ner-model'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466,
          "referenced_widgets": [
            "8f175bee01e54171964462a39acbff80",
            "20f102108e4d41c9bd5dbc85381ff6f4",
            "bb7b962d827f4fc19be619b87b4526e5",
            "615d0059c4864489bb209afb39ef0637",
            "3cad46a1ad274115b435100a56ad7a04",
            "d30ed96393724f0dba0957e0995aa790",
            "082bba2f93e548a18534dad97ec4c511",
            "af1b508a7fac49bba7fc215f3d6714c7",
            "e84e059f42f74eb19a4fb0b82d2c11c1",
            "2bc909535b5640e98e4594a91dc64117",
            "d190f659e6424374a859e77209371c74",
            "28519e00829b485a90ecb158a91dc393",
            "8740c5738a704a58bd9df8c11fe58053",
            "2dbf9294e38d4d99b297991b427e32de",
            "ce9f75b0e0244c5ca2c1bb3bb5c14fac",
            "f6376c8ec7b94869905733a3e53db1ec",
            "f3ab3550e08a41bdae74515ad801bfe5",
            "1d7f849d073b4eedb23e35c553aaf387",
            "8d448d025dac435f86a83a2cd6483d07",
            "171bac7a46c748038fc2c92a18464d29",
            "dbdf69434cae41b5a45c4626a48b9c8b",
            "9ea8e6bc257240fcb8a4fdb3dd0712d9"
          ]
        },
        "id": "DfqaGFlt1PIG",
        "outputId": "54e85df9-3862-4f78-97ba-df9791b95d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Mapping: {'B-ADDRESS': 0, 'B-EMAIL': 1, 'B-EXPERIENCE': 2, 'B-JOB': 3, 'B-LINKEDIN': 4, 'B-NAME': 5, 'I-ADDRESS': 6, 'I-JOB': 7, 'I-NAME': 8, 'O': 9}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/25986 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f175bee01e54171964462a39acbff80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/6497 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28519e00829b485a90ecb158a91dc393"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-2-bd6fe294a014>:84: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmugambimoses2\u001b[0m (\u001b[33mmugambimoses2-zindua-school\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250221_050835-x5lavgul</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mugambimoses2-zindua-school/huggingface/runs/x5lavgul' target=\"_blank\">./bert-ner-model</a></strong> to <a href='https://wandb.ai/mugambimoses2-zindua-school/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mugambimoses2-zindua-school/huggingface' target=\"_blank\">https://wandb.ai/mugambimoses2-zindua-school/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mugambimoses2-zindua-school/huggingface/runs/x5lavgul' target=\"_blank\">https://wandb.ai/mugambimoses2-zindua-school/huggingface/runs/x5lavgul</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5MXgIzho1Ov5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Pretrained BERT for Named Entity Recognition\n",
        "\n",
        "Use Hugging Face's transformers pipeline for NER."
      ],
      "metadata": {
        "id": "e9cx8pbfFteS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "sbb3c1oK8Quw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the pretrained BERT NER model\n",
        "bert_ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n",
        "\n",
        "# List of keywords that indicate a company or organization\n",
        "company_keywords = [\n",
        "    \"Solutions\", \"Institute\", \"Inc\", \"Ltd\", \"Limited\", \"Company\", \"Organization\",\n",
        "    \"Group\", \"Corporation\", \"Technologies\", \"Systems\", \"Consulting\", \"University\",\n",
        "    \"Enterprises\", \"Foundation\", \"Associates\", \"Partners\", \"Industries\", \"org\", \"traders\"\n",
        "]\n",
        "\n",
        "def extract_entities_bert(text):\n",
        "    entities = bert_ner(text)\n",
        "    extracted_info = {\n",
        "        \"APPLICANT_NAME\": [],\n",
        "        \"COMPANY\": [],\n",
        "        \"JOB_ROLE\": [],\n",
        "        \"SKILL\": [],\n",
        "        \"EDUCATION\": [],\n",
        "        \"CERTIFICATION\": [],\n",
        "        \"YEARS_EXPERIENCE\": [],\n",
        "        \"LOCATION\": [],\n",
        "        \"LINKEDIN\": []\n",
        "    }\n",
        "\n",
        "    # Map entity labels to desired categories\n",
        "    entity_mapping = {\n",
        "        \"PER\": \"APPLICANT_NAME\",\n",
        "        \"ORG\": \"COMPANY\",\n",
        "        \"JOB\": \"JOB_ROLE\",\n",
        "        \"SKILL\": \"SKILL\",\n",
        "        \"EDU\": \"EDUCATION\",\n",
        "        \"CERT\": \"CERTIFICATION\",\n",
        "        \"EXP\": \"YEARS_EXPERIENCE\",\n",
        "        \"LOC\": \"LOCATION\",\n",
        "        \"URL\": \"LINKEDIN\",\n",
        "    }\n",
        "\n",
        "    for entity in entities:\n",
        "        label = entity[\"entity_group\"]\n",
        "        word = entity[\"word\"]\n",
        "        mapped_label = entity_mapping.get(label)\n",
        "\n",
        "        if mapped_label:\n",
        "            if mapped_label == \"COMPANY\":\n",
        "                # Check if word contains any company-related keyword\n",
        "                if any(keyword.lower() in word.lower() for keyword in company_keywords):\n",
        "                    extracted_info[mapped_label].append(word)\n",
        "            else:\n",
        "                extracted_info[mapped_label].append(word)\n",
        "\n",
        "    # Convert lists to strings or keep them as lists if multiple entities exist\n",
        "    for key in extracted_info:\n",
        "        if len(extracted_info[key]) == 1:\n",
        "            extracted_info[key] = extracted_info[key][0]\n",
        "        elif len(extracted_info[key]) == 0:\n",
        "            extracted_info[key] = None\n",
        "\n",
        "    return extracted_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HiLbzX9FxAM",
        "outputId": "4dac962f-cc15-4f4a-c51a-0cb6d15ba288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract Additional Features\n",
        "\n",
        "Since BERT does not handle all entities well, I'll use regex for:\n",
        "\n",
        "Phone numbers, Emails, LinkedIn profiles"
      ],
      "metadata": {
        "id": "GfksBYJuGA47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_additional_info(text, extracted_info):\n",
        "    \"\"\"Extracts phone number, email, and LinkedIn profile from text.\"\"\"\n",
        "\n",
        "    # 🔹 Extract First Phone Number (Supports International Formats)\n",
        "    phone_pattern = re.compile(\n",
        "        r'(\\+?\\d{1,3}[-.\\s]?)?'  # Country code (optional)\n",
        "        r'(\\(?\\d{2,4}\\)?[-.\\s]?)?'  # Area code (optional)\n",
        "        r'\\d{3,4}[-.\\s]?\\d{3,4}[-.\\s]?\\d{0,4}'  # Main number\n",
        "    )\n",
        "\n",
        "    phone_match = phone_pattern.search(text)  # Extracts only the first match\n",
        "    if phone_match:\n",
        "        extracted_info[\"PHONE\"] = phone_match.group().strip()\n",
        "\n",
        "    # 🔹 Extract Email\n",
        "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "    email_match = re.search(email_pattern, text)  # Extracts only the first email\n",
        "    if email_match:\n",
        "        extracted_info[\"EMAIL\"] = email_match.group()\n",
        "\n",
        "    # 🔹 Extract LinkedIn Profile\n",
        "    linkedin_pattern = r'https?://(www\\.)?linkedin\\.com/(in|pub|profile)/[A-Za-z0-9_-]+'\n",
        "    linkedin_match = re.search(linkedin_pattern, text)  # Extracts only the first LinkedIn URL\n",
        "    if linkedin_match:\n",
        "        extracted_info[\"LINKEDIN\"] = linkedin_match.group()\n",
        "\n",
        "    return extracted_info\n"
      ],
      "metadata": {
        "id": "z8MBc5x2reP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Skills & Job Roles for Matching\n",
        "\n",
        "Use predefined job roles and skills for entity matching."
      ],
      "metadata": {
        "id": "qFnV_01wGOr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_keywords(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return [line.strip().lower() for line in file.readlines()]\n",
        "\n",
        "skills_list = load_keywords(\"/content/unique_skills.txt\")\n",
        "job_roles_list = load_keywords(\"/content/unique_job_roles.txt\")\n",
        "\n",
        "def match_keywords(text, keyword_list):\n",
        "    return [word for word in keyword_list if word in text.lower()]\n",
        "\n",
        "def extract_skills_job_roles(text, extracted_info):\n",
        "    extracted_info[\"SKILLS\"] = match_keywords(text, skills_list)\n",
        "    extracted_info[\"JOB_ROLE\"] = match_keywords(text, job_roles_list)\n",
        "    return extracted_info\n"
      ],
      "metadata": {
        "id": "Gl3df7T8GRMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_years_experience(text):\n",
        "    \"\"\"\n",
        "    Extracts years of experience from resume text using regex patterns.\n",
        "    \"\"\"\n",
        "    experience_patterns = [\n",
        "        r\"(\\d+)\\s*(?:\\+?\\s*years?|yrs|Yrs|yr|years of experience)\",  # \"5 years of experience\", \"10+ years\"\n",
        "        r\"(\\d+)-(\\d+)\",  # \"2015-2020\" (Calculate difference)\n",
        "        r\"since (\\d{4})\",  # \"since 2015\"\n",
        "        r\"(\\d{4}) to (\\d{4})\"  # \"2015 to 2020\"\n",
        "    ]\n",
        "\n",
        "    extracted_years = []\n",
        "\n",
        "    for pattern in experience_patterns:\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "        for match in matches:\n",
        "            if isinstance(match, tuple):  # Handling cases like \"2015-2020\"\n",
        "                if len(match) == 2:\n",
        "                    start, end = map(int, match)\n",
        "                    extracted_years.append(end - start)\n",
        "            else:\n",
        "                extracted_years.append(int(match))\n",
        "\n",
        "    # Get the max experience found\n",
        "    return max(extracted_years, default=\"Not Found\")\n",
        "\n"
      ],
      "metadata": {
        "id": "6eQuFyjzQ7cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complete Resume Parsing Pipeline\n",
        "\n",
        "Combine BERT NER + Regex + Keyword Matching:"
      ],
      "metadata": {
        "id": "rObT4kbVGecB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "def parse_resume(pdf_path):\n",
        "    # Step 1: Extract text from PDF\n",
        "    resume_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Step 2: Extract entities using BERT\n",
        "    extracted_info = extract_entities_bert(resume_text)\n",
        "\n",
        "    # Step 3: Extract additional details (phone, email, LinkedIn, certifications)\n",
        "    extracted_info = extract_additional_info(resume_text, extracted_info)\n",
        "\n",
        "    # Step 4: Match skills and job roles\n",
        "    extracted_info = extract_skills_job_roles(resume_text, extracted_info)\n",
        "\n",
        "    # Step 5: Extract Companies Worked For\n",
        "    extracted_info[\"COMPANIES_WORKED_FOR\"] = extract_companies(resume_text)\n",
        "\n",
        "    # Step 6: Extract Education Background\n",
        "    extracted_info[\"EDUCATION\"], extracted_info[\"EDUCATION_INSTITUTIONS\"] = extract_education(resume_text)\n",
        "\n",
        "    # ✅ Extract Years of Experience\n",
        "    extracted_info[\"YEARS_EXPERIENCE\"] = extract_years_experience(resume_text)\n",
        "\n",
        "    # Ensure all required fields exist\n",
        "    required_keys = [\n",
        "        \"APPLICANT_NAME\", \"JOB_ROLE\", \"PHONE\", \"EMAIL\", \"COMPANIES_WORKED_FOR\",\n",
        "        \"YEARS_EXPERIENCE\", \"SKILLS\", \"REFEREES\", \"LINKEDIN\", \"CERTIFICATIONS\",\n",
        "        \"EDUCATION\", \"EDUCATION_INSTITUTIONS\"\n",
        "    ]\n",
        "    extracted_info = {key: extracted_info.get(key, \"Not Found\") for key in required_keys}\n",
        "\n",
        "    # Clean Skills List\n",
        "    extracted_info[\"SKILLS\"] = clean_skills_list(extracted_info[\"SKILLS\"])\n",
        "\n",
        "    return extracted_info\n",
        "\n",
        "# 🔹 Extract companies based on patterns & BERT\n",
        "def extract_companies(text):\n",
        "    company_patterns = r\"\\b(?:Solutions|Institute|Company|Inc|Ltd|Limited|Corp|Technologies|Consulting|Industries|Systems|Enterprises)\\b\"\n",
        "\n",
        "    # ✅ Extract ORG entities from BERT (Fix: Use extract_entities_bert)\n",
        "    entities = extract_entities_bert(text)\n",
        "    company_names = entities.get(\"COMPANY\", [])  # Extract company names\n",
        "\n",
        "    # ✅ Extract based on patterns\n",
        "    pattern_matches = re.findall(r\"([A-Z][a-z]+(?:\\s[A-Z][a-z]+)*\\s\" + company_patterns + \")\", text)\n",
        "\n",
        "    # ✅ Combine & remove duplicates\n",
        "    companies = list(set((company_names if isinstance(company_names, list) else [company_names]) + pattern_matches))\n",
        "    return companies if companies else \"Not Found\"\n",
        "\n",
        "\n",
        "# 🔹 Extract education details based on degrees & institutions\n",
        "def extract_education(text):\n",
        "    degree_keywords = r\"\\b(?:Diploma|Certificate|Bachelor(?:'s)?|BSc|MSc|PhD|Master(?:'s)?|Doctorate|Associate|Engineering|MBA|BS|MS|BA|MA)\\b\"\n",
        "\n",
        "    # ✅ Extract ORG entities from BERT (universities are often tagged as ORG)\n",
        "    entities = extract_entities_bert(text)\n",
        "    institutions = entities.get(\"COMPANY\", [])  # Universities often get tagged as ORG\n",
        "\n",
        "    # ✅ Extract degrees from text\n",
        "    degrees = re.findall(degree_keywords, text, re.IGNORECASE)\n",
        "\n",
        "    return (\", \".join(set(degrees)) if degrees else \"Not Found\", institutions if institutions else \"Not Found\")\n",
        "\n",
        "# 🔹 Improved Clean Skills List\n",
        "def clean_skills_list(skills_list):\n",
        "    \"\"\"Remove noise from extracted skills, including stopwords and non-alphabetic characters.\"\"\"\n",
        "    stopwords = {\"the\", \"and\", \"in\", \"of\", \"to\", \"for\", \"on\", \"by\", \"with\", \"as\", \"at\", \"or\", \"an\", \"a\", \"is\", \"it\", \"be\"}\n",
        "\n",
        "    cleaned_skills = []\n",
        "    for skill in skills_list:\n",
        "        # Remove non-alphabetic characters (except spaces for multi-word skills)\n",
        "        skill = re.sub(r\"[^a-zA-Z\\s]\", \"\", skill)\n",
        "\n",
        "        # Normalize spacing and case\n",
        "        skill = skill.strip().lower()\n",
        "\n",
        "        # Skip stopwords and short words\n",
        "        if skill and skill not in stopwords and len(skill) > 2:\n",
        "            cleaned_skills.append(skill)\n",
        "\n",
        "    return cleaned_skills\n"
      ],
      "metadata": {
        "id": "e7_l24O9GgBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "def parse_resume(pdf_path):\n",
        "    # Step 1: Extract text from PDF\n",
        "    resume_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Step 2: Extract entities using BERT\n",
        "    extracted_info = extract_entities_bert(resume_text)\n",
        "\n",
        "    # ✅ Ensure Applicant Name Exists\n",
        "    if not extracted_info.get(\"APPLICANT_NAME\") or extracted_info[\"APPLICANT_NAME\"].strip() == \"\":\n",
        "        first_line = resume_text.strip().split(\"\\n\")[0]  # Get the first line\n",
        "        extracted_info[\"APPLICANT_NAME\"] = \" \".join(first_line.split()[:2])  # Extract first two words\n",
        "\n",
        "    # Step 3: Extract additional details (phone, email, LinkedIn, certifications)\n",
        "    extracted_info = extract_additional_info(resume_text, extracted_info)\n",
        "\n",
        "    # Step 4: Match skills and job roles\n",
        "    extracted_info = extract_skills_job_roles(resume_text, extracted_info)\n",
        "\n",
        "    # Step 5: Extract Companies Worked For\n",
        "    extracted_info[\"COMPANIES_WORKED_FOR\"] = extract_companies(resume_text)\n",
        "\n",
        "    # Step 6: Extract Education Background\n",
        "    extracted_info[\"EDUCATION\"], extracted_info[\"EDUCATION_INSTITUTIONS\"] = extract_education(resume_text)\n",
        "\n",
        "    # ✅ Extract Years of Experience\n",
        "    extracted_info[\"YEARS_EXPERIENCE\"] = extract_years_experience(resume_text)\n",
        "\n",
        "    # Ensure all required fields exist\n",
        "    required_keys = [\n",
        "        \"APPLICANT_NAME\", \"JOB_ROLE\", \"PHONE\", \"EMAIL\", \"COMPANIES_WORKED_FOR\",\n",
        "        \"YEARS_EXPERIENCE\", \"SKILLS\", \"REFEREES\", \"LINKEDIN\", \"CERTIFICATIONS\",\n",
        "        \"EDUCATION\", \"EDUCATION_INSTITUTIONS\"\n",
        "    ]\n",
        "    extracted_info = {key: extracted_info.get(key, \"Not Found\") for key in required_keys}\n",
        "\n",
        "    # Clean Skills List\n",
        "    extracted_info[\"SKILLS\"] = clean_skills_list(extracted_info[\"SKILLS\"])\n",
        "\n",
        "    return extracted_info\n"
      ],
      "metadata": {
        "id": "2nNXt8HK1Eo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on a sample resume\n",
        "parsed_data = parse_resume(\"/content/Moses Mugambi Data Analyst CV.pdf\")\n",
        "\n",
        "# Print each extracted entity on a new line\n",
        "print(\"\\nExtracted Resume Information:\")\n",
        "for key, value in parsed_data.items():\n",
        "    print(f\"{key}: {value if value != 'Not Found' else 'N/A'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFDTdc3Zizlp",
        "outputId": "d862fa4f-842d-4ad0-ea7e-205928aa5581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted Resume Information:\n",
            "APPLICANT_NAME: Moses Mu\n",
            "JOB_ROLE: ['electronics engineer', 'data scientist', 'engineer', 'scientist', 'it', 'engineering']\n",
            "PHONE: +254718695260\n",
            "EMAIL: mugambimoses2@gmail.com\n",
            "COMPANIES_WORKED_FOR: ['Technical University of Kenya']\n",
            "YEARS_EXPERIENCE: N/A\n",
            "SKILLS: ['excel', 'data analysis', 'science', 'excel', 'sql', 'machine learning', 'sql', 'python', 'work', 'act', 'point', 'tech', 'web', 'certificate', 'analysis', 'school', 'project', 'con', 'data visualization', 'grade', 'engineering', 'cal', 'based', 'professional', 'end', 'base', 'data', 'data s', 'pre', 'you', 'mary', 'adapt', 'control', 'high school', 'visual', 'time', 'view', 'chi', 'roll', 'visualization', 'world', 'one', 'per', 'ana', 'drive', 'mma', 'lea', 'experience', 'series analysis', 'google', 'line', 'mail', 'prof', 'scientific', 'series', 'table', 'team', 'engineer', 'model', 'ken', 'rev', 'rom', 'fun', 'apps', 'learning', 'data science', 'quality', 'review', 'las', 'ada', 'act']\n",
            "REFEREES: N/A\n",
            "LINKEDIN: None\n",
            "CERTIFICATIONS: N/A\n",
            "EDUCATION: Engineering, Certificate\n",
            "EDUCATION_INSTITUTIONS: Technical University of Kenya\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the Model for Deployment\n",
        "\n",
        "Save the BERT pipeline in joblib and pickle for future use."
      ],
      "metadata": {
        "id": "WDJnI316G-mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pickle\n",
        "\n",
        "# Save BERT NER pipeline using joblib\n",
        "joblib.dump(bert_ner, \"bert_resume_parser1.joblib\")\n",
        "\n",
        "# Save using pickle\n",
        "with open(\"bert_resume_parser1.pkl\", \"wb\") as f:\n",
        "    pickle.dump(bert_ner, f)\n"
      ],
      "metadata": {
        "id": "GODFSAQCHB1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Use the Saved Model\n",
        "\n",
        "Reload the model when deploying:"
      ],
      "metadata": {
        "id": "K3Cnkji6HMtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model from joblib\n",
        "bert_ner = joblib.load(\"bert_resume_parser.joblib\")\n",
        "\n",
        "# Load model from pickle\n",
        "with open(\"bert_resume_parser.pkl\", \"rb\") as f:\n",
        "    bert_ner = pickle.load(f)\n",
        "\n",
        "# Test with a new resume\n",
        "new_parsed_data = parse_resume(\"/content/Moses Mugambi Data Analyst CV.pdf\")\n",
        "# Print each extracted entity on a new line\n",
        "print(\"\\nExtracted Resume Information:\")\n",
        "for key, value in new_parsed_data.items():\n",
        "    print(f\"{key}: {value if value != 'Not Found' else 'N/A'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5ZoI28cHRXV",
        "outputId": "79234c1f-1b4d-4755-b23d-0e66c8771508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted Resume Information:\n",
            "APPLICANT_NAME: Moses Mu\n",
            "JOB_ROLE: ['data scientist']\n",
            "PHONE: 2547186952\n",
            "EMAIL: mugambimoses2@gmail.com\n",
            "COMPANIES_WORKED_FOR: ['Technical University of Kenya']\n",
            "YEARS_EXPERIENCE: N/A\n",
            "SKILLS: ['excel', 'tri', 'data analysis', 'science', 'out', 'sql', 'mac', 'pro', 'machine learning', 'python', 'work', 'act', 'point', 'tech', 'web', 'certificate', 'analysis', 'school', 'project', 'con', 'data visualization', 'grade', 'engineering', 'cal', 'based', 'professional', 'end', 'base', 'data', 'data s', 'pre', 'you', 'mary', 'adapt', 'control', 'high school', 'visual', 'time', 'view', 'chi', 'roll', 'visualization', 'world', 'one', 'per', 'ana', 'drive', 'mma', 'lea', 'experience', 'series analysis', 'google', 'line', 'mail', 'prof', 'scientific', 'series', 'table', 'team', 'engineer', 'model', 'ken', 'rev', 'rom', 'fun', 'apps', 'learning', 'data science', 'quality', 'review', 'las', 'ada', 'act']\n",
            "REFEREES: N/A\n",
            "LINKEDIN: None\n",
            "CERTIFICATIONS: N/A\n",
            "EDUCATION: Engineering, Certificate\n",
            "EDUCATION_INSTITUTIONS: Technical University of Kenya\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement CV Rating Based on Job Description\n",
        "**Steps:**\n",
        "\n",
        "Extract key requirements from the job description:\n",
        "\n",
        "Skills (Python, SQL, Machine Learning, etc.)\n",
        "Experience Level (years of experience)\n",
        "Education Requirements\n",
        "Certifications (AWS, PMP, etc.)\n",
        "Compare Resume vs. Job Description\n",
        "\n",
        "Match extracted skills, experience, and education\n",
        "Assign weights to each category\n",
        "Score the CV based on how well it matches the job"
      ],
      "metadata": {
        "id": "REwrXUHVTfPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract Key Information from Job Description\n",
        "First, create a function to extract keywords from a job description using NER, Regex, and NLP."
      ],
      "metadata": {
        "id": "7SbKlVabTtYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Load Spacy NLP model\n",
        "\n",
        "def extract_job_requirements(job_text):\n",
        "    \"\"\"\n",
        "    Extracts key requirements (skills, experience, education) from job description.\n",
        "    \"\"\"\n",
        "    doc = nlp(job_text)\n",
        "\n",
        "    # Extract education (Bachelor, Master, PhD)\n",
        "    education_levels = [\"bachelor\", \"master\", \"phd\", \"degree\", \"diploma\"]\n",
        "    extracted_education = [token.text for token in doc if token.text.lower() in education_levels]\n",
        "\n",
        "    # Extract required skills using simple regex (you can enhance this with a skill dataset)\n",
        "    skill_pattern = r\"\\b[A-Za-z+#.]+\\b\"\n",
        "    extracted_skills = re.findall(skill_pattern, job_text)\n",
        "\n",
        "    # Extract years of experience\n",
        "    experience = re.findall(r\"(\\d+)\\s*(?:\\+?\\s*years?|yrs|years of experience)\", job_text)\n",
        "\n",
        "    job_requirements = {\n",
        "        \"EDUCATION_REQUIRED\": list(set(extracted_education)),\n",
        "        \"SKILLS_REQUIRED\": list(set(extracted_skills)),\n",
        "        \"EXPERIENCE_REQUIRED\": max(map(int, experience), default=0)\n",
        "    }\n",
        "\n",
        "    return job_requirements\n"
      ],
      "metadata": {
        "id": "nUbG5DfNTwPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score the Resume Against the Job Description\n",
        "Now, compare the extracted resume details vs. job description requirements."
      ],
      "metadata": {
        "id": "Siwog_hTT0Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rate_cv(resume_data, job_requirements):\n",
        "    \"\"\"\n",
        "    Scores a resume based on how well it matches the job description.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    total_weight = 0\n",
        "\n",
        "    # ✅ Match Skills\n",
        "    resume_skills = set(resume_data.get(\"SKILLS\", []))\n",
        "    job_skills = set(job_requirements.get(\"SKILLS_REQUIRED\", []))\n",
        "\n",
        "    skill_match = len(resume_skills & job_skills) / max(1, len(job_skills))  # % match\n",
        "    score += skill_match * 40  # Skills have 40% weight\n",
        "    total_weight += 40\n",
        "\n",
        "    # ✅ Match Education Level\n",
        "    resume_edu = set(resume_data.get(\"EDUCATION\", []))\n",
        "    job_edu = set(job_requirements.get(\"EDUCATION_REQUIRED\", []))\n",
        "\n",
        "    education_match = 1 if resume_edu & job_edu else 0  # Full match if any degree matches\n",
        "    score += education_match * 20  # Education has 20% weight\n",
        "    total_weight += 20\n",
        "\n",
        "    # ✅ Match Experience\n",
        "    resume_exp = resume_data.get(\"YEARS_EXPERIENCE\", 0)\n",
        "    job_exp = job_requirements.get(\"EXPERIENCE_REQUIRED\", 0)\n",
        "\n",
        "    experience_match = min(resume_exp / max(1, job_exp), 1)  # Cap at 100%\n",
        "    score += experience_match * 30  # Experience has 30% weight\n",
        "    total_weight += 30\n",
        "\n",
        "    # ✅ Bonus for Certifications (if applicable)\n",
        "    cert_bonus = 10  # Bonus for having extra certifications\n",
        "    score += cert_bonus\n",
        "    total_weight += 10\n",
        "\n",
        "    # Normalize Score\n",
        "    final_score = (score / total_weight) * 100  # Convert to percentage\n",
        "\n",
        "    return round(final_score, 2)\n"
      ],
      "metadata": {
        "id": "kP-t_JLMT09t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modify parse_resume to Include CV Rating\n",
        "Now, integrate job analysis + CV rating into your main function:"
      ],
      "metadata": {
        "id": "giZCF3N8T5s2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_resume_and_rate(pdf_path, job_description):\n",
        "    \"\"\"\n",
        "    Parses a resume, extracts details, and rates it against a job description.\n",
        "    \"\"\"\n",
        "    resume_text = extract_text_from_pdf(pdf_path)\n",
        "    resume_data = parse_resume(resume_text)  # Extract resume details\n",
        "\n",
        "    # Extract Job Requirements\n",
        "    job_requirements = extract_job_requirements(job_description)\n",
        "\n",
        "    # Score Resume\n",
        "    cv_score = rate_cv(resume_data, job_requirements)\n",
        "\n",
        "    # Add score to extracted resume data\n",
        "    resume_data[\"CV_SCORE\"] = cv_score\n",
        "    return resume_data\n"
      ],
      "metadata": {
        "id": "rSlr1qncT7Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "\n",
        "sample_resume_path = \"/content/Moses Mugambi Data Analyst CV.pdf\"\n",
        "\n",
        "try:\n",
        "    with pdfplumber.open(sample_resume_path) as pdf:\n",
        "        print(\"✅ PDF file opened successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"❌ FileNotFoundError: The file path is incorrect.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Another error occurred: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkHLvDVgV1Ii",
        "outputId": "2c53a6a8-3415-49d7-8807-4384c8bd5449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ PDF file opened successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Job Description\n",
        "job_description = \"\"\"\n",
        "We are looking for a Data Scientist with 5+ years of experience in Machine Learning, Python, and SQL.\n",
        "The ideal candidate should have a Master's degree in Computer Science or a related field.\n",
        "Familiarity with cloud platforms (AWS, GCP) is a plus.\n",
        "\"\"\"\n",
        "\n",
        "sample_resume_path = \"/content/Moses Mugambi Data Analyst CV.pdf\"  # Provide the actual file path\n",
        "print(type(sample_resume_path))  # Should be <class 'str'>\n",
        "\n",
        "def debug_parse_resume(path):\n",
        "    print(f\"📂 Trying to open file at: {repr(path)}\")\n",
        "    with pdfplumber.open(path) as pdf:\n",
        "        return \"✅ Opened successfully!\"\n",
        "\n",
        "debug_parse_resume(sample_resume_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "nIHz1D8sUCvw",
        "outputId": "b4ac530a-fd11-4556-ee22-1a49c72f36ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "📂 Trying to open file at: '/content/Moses Mugambi Data Analyst CV.pdf'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'✅ Opened successfully!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def extract_resume_text(pdf_path):\n",
        "    \"\"\"Extracts text from the resume PDF.\"\"\"\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            text = \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
        "        return text if text else \"No text found.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error reading file: {e}\"\n",
        "\n",
        "def summarize_resume(resume_text):\n",
        "    \"\"\"Extracts key sections: skills, experience, education from resume text.\"\"\"\n",
        "\n",
        "    summary = {}\n",
        "\n",
        "    # Extract Experience (Look for 'Experience', 'Work', 'Projects')\n",
        "    experience_match = re.search(r\"(experience|work history|projects):?\\s*(.+)\", resume_text, re.IGNORECASE)\n",
        "    summary[\"Experience\"] = experience_match.group(2) if experience_match else \"Not Found\"\n",
        "\n",
        "    # Extract Skills (Look for 'Skills', 'Technical Skills', etc.)\n",
        "    skills_match = re.search(r\"(skills|technical skills):?\\s*(.+)\", resume_text, re.IGNORECASE)\n",
        "    summary[\"Skills\"] = skills_match.group(2) if skills_match else \"Not Found\"\n",
        "\n",
        "    # Extract Education\n",
        "    education_match = re.search(r\"(education|academic background):?\\s*(.+)\", resume_text, re.IGNORECASE)\n",
        "    summary[\"Education\"] = education_match.group(2) if education_match else \"Not Found\"\n",
        "\n",
        "    # Create final description\n",
        "    resume_description = (\n",
        "        f\"Experience: {summary['Experience']}\\n\"\n",
        "        f\"Skills: {summary['Skills']}\\n\"\n",
        "        f\"Education: {summary['Education']}\\n\"\n",
        "    )\n",
        "\n",
        "    return resume_description\n",
        "\n",
        "def compare_resume_with_job(resume_text, job_description):\n",
        "    \"\"\"Compares resume description with job description using TF-IDF similarity.\"\"\"\n",
        "\n",
        "    documents = [resume_text, job_description]\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarity_score = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
        "\n",
        "    return round(similarity_score * 100, 2)  # Convert to percentage\n",
        "\n",
        "# === 🚀 Test the Code on a Sample Resume & Job Description ===\n",
        "resume_pdf_path = \"/content/Moses Mugambi Data Analyst CV.pdf\"  # 🔹 Replace with actual file path\n",
        "job_description = \"\"\"\n",
        "We are looking for a Data Scientist with expertise in Python, SQL, and machine learning.\n",
        "The ideal candidate should have experience in data visualization, NLP, and time series analysis.\n",
        "\"\"\"\n",
        "\n",
        "# 1️⃣ Extract Resume Text\n",
        "resume_text = extract_resume_text(resume_pdf_path)\n",
        "print(\"\\n📄 Extracted Resume Text:\\n\", resume_text[:], \"...\")  # Print first 500 chars\n",
        "\n",
        "# 2️⃣ Summarize Resume\n",
        "resume_summary = summarize_resume(resume_text)\n",
        "print(\"\\n📌 Resume Summary:\\n\", resume_summary)\n",
        "\n",
        "# 3️⃣ Compare Resume with Job Description\n",
        "match_score = compare_resume_with_job(resume_summary, job_description)\n",
        "print(\"\\n✅ CV Match Score:\", match_score, \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q31OT2KUYoVb",
        "outputId": "1675d463-d241-443e-a8f9-12dbdf26145f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📄 Extracted Resume Text:\n",
            " Moses Mugambi\n",
            "Data Scientist\n",
            "Email: mugambimoses2@gmail.com | Phone: +254718695260\n",
            "LinkedIn: linkedin.com/in/moses-mugambi-njeru | GitHub: github.com/mk7890\n",
            "Professional Summary\n",
            "Motivated Data Scientist with expertise in Python, data analysis, visualizations, and machine\n",
            "learning. Proficient in SQL, Tableau, Excel, and web scraping. Experienced in predictive\n",
            "modelling, clustering, and NLP. Passionate about deriving insights from data to solve real-world\n",
            "problems.\n",
            "Work Experience\n",
            "• Resume Parser. Built an NLP pipeline for extracting key resume details.\n",
            "• Google Play Store Apps & YouTube Analysis: Data-driven insights using SQL, Pandas,\n",
            "and visualizations.\n",
            "• Machine Learning Projects: Implemented Regression, Classification, Clustering, PCA,\n",
            "and CNNs.\n",
            "• Time Series Analysis: Forecasted air quality trends using statistical models.\n",
            "• NLP Sentiment Analysis: Analysed customer reviews for sentiment trends.\n",
            "• Voice-Controlled Calculator: Developed a Python-based scientific calculator with speech\n",
            "recognition functionality.\n",
            "Skills\n",
            "• Hard Skills: Python (Pandas, NumPy, Scikit-Learn), SQL, Tableau, Machine Learning,\n",
            "NLP, Web Scraping, Data Visualization.\n",
            "• Soft Skills: Problem-Solving, Analytical Thinking, Communication, Teamwork,\n",
            "Adaptability.\n",
            "Education\n",
            "Data Science Full - Time\n",
            "Zindua School, September 2024 – February 2025\n",
            "B.Sc. Electrical and Electronics Engineering\n",
            "The Technical University of Kenya, 2013 – 2018 : Second Upper Class Honours\n",
            "High School Certificate\n",
            "Moi High School Mbiruri, 2008 – 2011 : Grade A plain of 81 points ...\n",
            "\n",
            "📌 Resume Summary:\n",
            " Experience: d in predictive\n",
            "Skills: • Hard Skills: Python (Pandas, NumPy, Scikit-Learn), SQL, Tableau, Machine Learning,\n",
            "Education: Data Science Full - Time\n",
            "\n",
            "\n",
            "✅ CV Match Score: 22.89 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model"
      ],
      "metadata": {
        "id": "VOzehkWIj1r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pickle\n",
        "\n",
        "# Save BERT NER pipeline using joblib\n",
        "joblib.dump(bert_ner, \"bert_resume_parserCVrate.joblib\")\n",
        "\n",
        "# Save using pickle\n",
        "with open(\"bert_resume_parserCVrate.pkl\", \"wb\") as f:\n",
        "    pickle.dump(bert_ner, f)"
      ],
      "metadata": {
        "id": "oRNRYsn99Fxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the Model using Pickle (.pkl)"
      ],
      "metadata": {
        "id": "8-vD3lRYkD2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample job description for training the model\n",
        "job_description = \"\"\"\n",
        "We are looking for a Data Scientist with expertise in Python, SQL, and machine learning.\n",
        "The ideal candidate should have experience in data visualization, NLP, and time series analysis.\n",
        "\"\"\"\n",
        "\n",
        "# Train the TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit([job_description])  # Fit on sample job description\n",
        "\n",
        "# Save the vectorizer model using Pickle\n",
        "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "\n",
        "print(\"✅ Model saved as tfidf_vectorizer.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__uFQVN4kEjE",
        "outputId": "4d82004e-26cf-4461-ad2f-676da50b40f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model saved as tfidf_vectorizer.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and using the model"
      ],
      "metadata": {
        "id": "trWdMAuXkf9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "with open(\"tfidf_vectorizer.pkl\", \"rb\") as f:\n",
        "    loaded_vectorizer = pickle.load(f)\n",
        "\n",
        "# Test with new resume text\n",
        "resume_text = \"\"\"\n",
        "Data Scientist skilled in Python, SQL, and NLP. Experienced in machine learning and time series analysis.\n",
        "\"\"\"\n",
        "job_description_vector = loaded_vectorizer.transform([job_description])\n",
        "resume_vector = loaded_vectorizer.transform([resume_text])\n",
        "\n",
        "# Compute similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarity_score = cosine_similarity(resume_vector, job_description_vector)[0][0]\n",
        "\n",
        "print(\"✅ CV Match Score:\", round(similarity_score * 100, 2), \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W8uI61Ckhkn",
        "outputId": "8d05f380-05b4-48f6-ecb0-247b24756ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ CV Match Score: 76.8 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "JOBLIB"
      ],
      "metadata": {
        "id": "WQAeXthdj3zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the model with Joblib\n",
        "joblib.dump(vectorizer, \"tfidf_vectorizer.joblib\")\n",
        "\n",
        "print(\"✅ Model saved as tfidf_vectorizer.joblib\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxLx87QJj497",
        "outputId": "1cf26139-dd7b-4fcf-be87-751e558c96b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model saved as tfidf_vectorizer.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving a Hugging Face Transformers Model\n",
        "For BERT-based NER, your model likely consists of:\n",
        "\n",
        "A Transformer model (AutoModelForTokenClassification or pipeline(\"ner\")).\n",
        "A Tokenizer (AutoTokenizer).\n",
        "Any additional preprocessing logic.\n",
        "\n",
        "1. Save using joblib (Recommended)\n",
        "joblib is better for saving large models because it handles NumPy arrays efficiently."
      ],
      "metadata": {
        "id": "tTknD8dMlsml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the Hugging Face pipeline (if not already done)\n",
        "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "\n",
        "# Save the model using joblib\n",
        "joblib.dump(ner_pipeline, \"bert_ner_pipelineCVrate.joblib\")\n",
        "\n",
        "print(\"✅ BERT NER model saved successfully using joblib!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w8GYVg7lzLK",
        "outputId": "39d36c45-e6a6-4dd3-cedd-9f3c0127f4fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ BERT NER model saved successfully using joblib!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Save using pickle (Alternative)\n",
        "pickle works but is slower and less efficient for large models."
      ],
      "metadata": {
        "id": "dSJ81YzfmFqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the pipeline with pickle\n",
        "with open(\"bert_ner_pipelineCVrate.pkl\", \"wb\") as f:\n",
        "    pickle.dump(ner_pipeline, f)\n",
        "\n",
        "print(\"✅ BERT NER model saved successfully using pickle!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6nm0CdCmFUg",
        "outputId": "ff3adea6-42dd-47d7-80ba-21ca61173c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ BERT NER model saved successfully using pickle!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Loading the Model\n",
        "\n",
        "To load the model and use it for inference:"
      ],
      "metadata": {
        "id": "20vb6qOVmQVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load from joblib\n",
        "ner_pipeline = joblib.load(\"bert_ner_pipeline.joblib\")\n",
        "\n",
        "# Test on a sample text\n",
        "text = \"John Doe is a data scientist at Google, working on NLP.\"\n",
        "result = ner_pipeline(text)\n",
        "\n",
        "# Print each entity on a separate line\n",
        "print(\"NER Output:\")\n",
        "for entity in result:\n",
        "    print(f\"Entity: {entity['word']}, Label: {entity.get('entity_group', entity.get('entity'))}, Score: {entity['score']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7hpExH0mUqG",
        "outputId": "5adf1cde-c5eb-44c7-f781-971a85ceac5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NER Output:\n",
            "Entity: John, Label: I-PER, Score: 0.9996\n",
            "Entity: Do, Label: I-PER, Score: 0.9993\n",
            "Entity: ##e, Label: I-PER, Score: 0.9965\n",
            "Entity: Google, Label: I-ORG, Score: 0.9990\n",
            "Entity: NL, Label: I-MISC, Score: 0.6309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Model to Google drive"
      ],
      "metadata": {
        "id": "0xfJe23AnlgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iffr6UojnnmS",
        "outputId": "f7506ff7-aa1e-4f6b-ea43-5ffaaa1f70f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Source path (where the model is saved in Colab)\n",
        "source_path = \"/content/bert_ner_pipeline.joblib\"\n",
        "\n",
        "# Destination path (inside Google Drive)\n",
        "destination_path = \"/content/drive/MyDrive/bert_ner_pipeline_copy.joblib\"\n",
        "\n",
        "# Copy the file instead of moving\n",
        "shutil.copy(source_path, destination_path)\n",
        "\n",
        "print(f\"Model copied to {destination_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8ak7Lzlo8rN",
        "outputId": "2f6efd48-c99d-4a19-bbd3-1640d345ac52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model copied to /content/drive/MyDrive/bert_ner_pipeline_copy.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Source path (where the model is saved in Colab)\n",
        "source_path = \"/content/bert_resume_parser.pkl\"\n",
        "\n",
        "# Destination path (inside Google Drive)\n",
        "destination_path = \"/content/drive/MyDrive/bert_ner_pipeline_copy.pkl\"\n",
        "\n",
        "# Copy the file instead of moving\n",
        "shutil.copy(source_path, destination_path)\n",
        "\n",
        "print(f\"Model copied to {destination_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go0J_aylpFhv",
        "outputId": "7313d00a-64d1-493c-ef22-2edebfb45cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model copied to /content/drive/MyDrive/bert_ner_pipeline_copy.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "deploy the BERT-based NER model as a web app using Streamlit. The app will:\n",
        "\n",
        "✅ Allow users to upload a resume (PDF file)\n",
        "✅ Take a job description as input\n",
        "✅ Extract entities from the resume (NER output)\n",
        "✅ Compare it with the job description & provide a CV score\n",
        "\n",
        "📌 Deployment Approach\n",
        "Framework: Streamlit (Fast and simple UI)\n",
        "Backend: Uses Hugging Face Transformers for NER\n",
        "Storage: No need for a database, process files in memory\n",
        "Deployment Options: Streamlit Cloud, Hugging Face Spaces, or Render\n",
        "\n",
        "🚀 Steps to Deploy\n",
        "- Prepare the Model\n",
        "- Load the BERT-based NER model (joblib or pickle)\n",
        "- Use pdfplumber to extract text from the PDF resume\n",
        "- Build the Streamlit App\n",
        "- Upload PDF\n",
        "- Enter Job Description\n",
        "- Extract Named Entities from the resume\n",
        "- Compute a CV Score based on entity matching\n",
        "- Deploy on Streamlit Cloud or Hugging Face Spaces\n",
        "\n",
        "# 📌 Streamlit App Code (deployable)"
      ],
      "metadata": {
        "id": "u47e-KN9qnA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import joblib\n",
        "import pdfplumber\n",
        "from transformers import pipeline\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "# Load the NER model (pre-trained BERT)\n",
        "model = joblib.load(\"/content/bert_ner_pipeline.joblib\")  # Change path if needed\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_file) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text.strip()\n",
        "\n",
        "# Function to extract entities\n",
        "def extract_named_entities(text):\n",
        "    result = model(text)\n",
        "    return result  # List of entities\n",
        "\n",
        "# Function to score resume based on job description match\n",
        "def compute_cv_score(entities, job_desc):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    job_tokens = set([token.lemma_ for token in nlp(job_desc.lower()) if not token.is_stop])\n",
        "    entity_words = set([re.sub(r\"[^a-zA-Z0-9]\", \"\", ent[\"word\"].lower()) for ent in entities])\n",
        "\n",
        "    common_words = job_tokens.intersection(entity_words)\n",
        "    score = len(common_words) / len(job_tokens) * 100 if job_tokens else 0\n",
        "    return round(score, 2)\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"📄 AI-Powered Resume Parser & CV Scorer\")\n",
        "st.write(\"Upload your resume and enter the job description to analyze your fit.\")\n",
        "\n",
        "# Upload Resume\n",
        "uploaded_file = st.file_uploader(\"Upload Resume (PDF)\", type=[\"pdf\"])\n",
        "\n",
        "# Enter Job Description\n",
        "job_desc = st.text_area(\"Enter Job Description\")\n",
        "\n",
        "# Process when button is clicked\n",
        "if st.button(\"Analyze Resume\"):\n",
        "    if uploaded_file and job_desc:\n",
        "        resume_text = extract_text_from_pdf(uploaded_file)\n",
        "        st.subheader(\"Extracted Resume Text\")\n",
        "        st.write(resume_text[:1000] + \"...\")  # Show only first 1000 characters\n",
        "\n",
        "        # Extract Named Entities\n",
        "        entities = extract_named_entities(resume_text)\n",
        "        st.subheader(\"Extracted Named Entities\")\n",
        "        for entity in entities:\n",
        "            st.write(f\"**Entity:** {entity['word']} | **Label:** {entity['entity']} | **Score:** {entity['score']:.4f}\")\n",
        "\n",
        "        # Compute CV Score\n",
        "        cv_score = compute_cv_score(entities, job_desc)\n",
        "        st.subheader(f\"🔍 CV Match Score: {cv_score}%\")\n",
        "        st.progress(cv_score / 100)\n",
        "\n",
        "    else:\n",
        "        st.warning(\"Please upload a resume and enter a job description!\")\n"
      ],
      "metadata": {
        "id": "dE8BJL4rqyYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Deploy\n",
        "\n",
        "1️⃣ Run Locally\n",
        "\n",
        "Save the script as app.py\n",
        "\n",
        "Install dependencies:"
      ],
      "metadata": {
        "id": "yjB9_gRbq5Bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit transformers pdfplumber spacy joblib\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "2ngrYqVAq75E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bash"
      ],
      "metadata": {
        "id": "TB-fQFxJrAq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit run app.py\n"
      ],
      "metadata": {
        "id": "amjza2Eaq_x4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}